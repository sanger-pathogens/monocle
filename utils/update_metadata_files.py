#!/usr/bin/env python3

import copy
import json
import os
import re

import yaml

"""
Done:
database schema
metadata API SQL queries also have lists of db column names.
metadata API config.json file (used with Pandas for spreadsheet validation and loading; also referred to by metadata download code)
metadata API metadata data model
metadata API in silico data model
metadata API QC data model
tests
dashboard API openapi.yml file (OpenAPI spec. for endpoints)
metadata API openapi.yml file (OpenAPI spec. for endpoints)
"""


class UpdateMetadataFiles:
    def __init__(self):
        self.indent = "    "
        self.root_path = f"{os.path.dirname(__file__)}/.."
        self.map_config_dict = dict(metadata="metadata", in_silico_data="in silico", qc_data="qc data")
        self.config_additional_section_keys = ["spreadsheet_header_row_position", "upload_validation_enabled"]
        self.yml2json = [
            ["Metadata", "metadata", "metadata"],
            ["InSilicoData", "in_silico_data", "insilicodata"],
            ["QCData", "qc_data", "qcdata"],
        ]
        self.yml_field_patterns = [
            ["MetadataFieldName", "metadata"],
            ["InSilicoFieldName", "in_silico_data"],
            ["QCDataFieldName", "qc_data"],
        ]

    def var_type_heuristic(self, data):
        """Guesses a Python variable type based of a definition in config.json."""
        if "regex" in data:
            if data["regex"] == "^[1]?[0-9]?[0-9]\\.[0-9][0-9]?$":
                return "float"
        return "str"

    def var_comment_heuristic(self, data):
        """Tries to construct a comment for a variable definition based on config.json."""
        comments = []
        if "mandatory" in data and data["mandatory"]:
            comments.append("mandatory")
        else:
            comments.append("optional")
        if "regex_validation_message" in data:
            comments.append(data["regex_validation_message"])
        if len(comments) == 0:
            return ""
        return "  # " + "; ".join(comments)

    def generate_dataclass_file(self, data, class_name, filename):
        """Generated a Python dataclass file."""
        autogeneration_note = self.get_autogeneration_note("FILE")
        output = f"from dataclasses import dataclass\n\n{autogeneration_note}\n\n@dataclass\nclass {class_name}:\n"
        for (k, v) in data["spreadsheet_definition"].items():
            var_type = self.var_type_heuristic(v)
            var_comment = self.var_comment_heuristic(v)
            output += f"{self.indent}{k}: {var_type}{var_comment}\n"
        with open(filename, "w") as output_file:
            _ = output_file.write(output)

    def pad(self, text, num):
        """Left-pads every line in a multi-line string with the given number of indents."""
        return self.indent * num + text.replace("\n", "\n" + self.indent * num)

    def chunk_text(self, sep, list, max):
        """Turns a list into a multi-line string with a given maximum of list items per line."""
        ret = self.indent
        for num, t in enumerate(list):
            if num > 0:
                ret += sep
                if num % max == 0 and num + 1 < len(list):
                    ret = ret.rstrip()
                    ret += "\n" + self.indent
            ret += t
        return ret

    def skip_lines_until(self, pattern, lines):
        """Skips through a list of strings until it finds one matching a given pattern.
        Removes all lines from the list until then, including the one that matches the pattern.
        Returns the line that matches the pattern.
        """
        p = re.compile(pattern)
        while len(lines) > 0:
            line = lines.pop(0)
            if p.match(line):
                return line
        return ""

    def get_autogeneration_note(self, area):
        """Creates a code comment to warn about auto-generated code."""
        return f"# THIS {area} IS AUTO-GENERATED BY utils/update_metadata_files.py, DO NOT EDIT MANUALLY!\n"

    def update_database_definition(self, data, filename):
        """Updates an SQL table definition file.

        NOTE: This could use some additional metadata in config.json, eg unusual MySQL types, ranges etc
        """
        with open(filename, "r") as in_file:
            lines = in_file.readlines()

        new_code = ""
        p = re.compile(r"^\s*CREATE TABLE .*$")
        while len(lines) > 0:
            line = lines.pop(0)
            out = line
            if p.match(line):
                out += "  " + self.get_autogeneration_note("TABLE DEFINITION")
                for (k, v) in data["spreadsheet_definition"].items():
                    row = f"  `{k}` "
                    if self.var_type_heuristic(v) == "float":
                        row += "DECIMAL(5,2) UNSIGNED"
                    elif "max_length" in v:
                        row += f"VARCHAR({v['max_length']})"
                    else:
                        row += "int(11)"
                    if "mandatory" in v and v["mandatory"]:
                        row += " NOT NULL"
                    else:
                        row += " DEFAULT NULL"
                    row += ",\n"
                    out += row
                out += "  # END OF AUTO_GENERATED SECTION\n"
                out += self.skip_lines_until(r"^.*PRIMARY KEY.*$", lines)
            new_code += out

        with open(filename, "w") as output_file:
            _ = output_file.write(new_code)

    def update_test_data(self, data, test_data_file_path):
        """Updates the test_data.py file if required.
        Uses as much of the existing test values as possible.
        """
        if not os.path.exists(test_data_file_path):
            print(f"Skipping non-existing file {test_data_file_path}")
            return
        with open(test_data_file_path) as test_data_file:
            lines = test_data_file.readlines()
        output = ""
        p_dict = re.compile(r"^(TEST_\S+_DICT)\s*=\s*dict\s*\(\S*$")
        p_key_value = re.compile(r"^\s*(\S+?)\s*=.*\s*\"(.*?)\"\s*,{0,1}\s*$")
        p_end_of_definition = re.compile(r"^\s*\)\S*$")
        while len(lines) > 0:
            line = lines.pop(0)
            m = p_dict.match(line)
            if not m:
                output += line
                continue
            var_name = m.group(1)
            d = {}
            while len(lines) > 0:
                row = lines.pop(0)
                n = p_key_value.match(row)
                if n:
                    d[n.group(1)] = n.group(2)
                if p_end_of_definition.match(row):
                    break
            if var_name in ["TEST_SAMPLE_1_DICT", "TEST_SAMPLE_2_DICT"]:
                c = data["metadata"]
            elif var_name in ["TEST_LANE_IN_SILICO_1_DICT", "TEST_LANE_IN_SILICO_2_DICT"]:
                c = data["in_silico_data"]
            elif var_name in ["TEST_LANE_QC_DATA_1_DICT", "TEST_LANE_QC_DATA_2_DICT"]:
                c = data["qc_data"]
            # Remove obsolete fields
            for k in list(d.keys()):
                if k not in c["spreadsheet_definition"]:
                    print(f"Removing {k} from test data")
                    d.pop(k)
            # Add new fields
            for k in c["spreadsheet_definition"].keys():
                if k not in d:
                    print(f"Adding {k} to test data")
                    d[k] = str(len(d) + 1)
            output += line
            for k, v in d.items():
                output += f'{self.indent}{k}="{v}",\n'
            output += ")\n"
        with open(test_data_file_path, "w") as output_file:
            _ = output_file.write(output)

    def update_metadata_tests(self, data, tests_path):
        """Updates tests that use data from config files."""
        self.update_test_data(data, f"{tests_path}/test_data.py")

    def update_yml_field_patterns(self, y, data):
        """Updates YAML field patterns."""
        for (yk, jk) in self.yml_field_patterns:
            json_keys = list(data[jk]["spreadsheet_definition"].keys())
            y["components"]["schemas"][yk]["pattern"] = "^" + "|".join(json_keys) + "$"

    def update_yml_field_list(self, y, j):
        y["properties"].clear()
        y["required"].clear()
        json_keys = list(j.keys())
        for k in json_keys:
            y["properties"][k] = {"$ref": "#/components/schemas/DownloadField"}
        y["required"] = json_keys

    def update_dash_yml(self, data, file_path):
        """Updates the dash openapi.yml file with various properties.
        TODO: default/required fields need to be annotated in upstream config and set here
        """
        if not os.path.exists(file_path):
            print(f"Skipping: {file_path} does not exist")
            return
        with open(file_path) as file:
            y = yaml.safe_load(file)

        self.update_yml_field_patterns(y, data)

        for (yk, jk, _) in self.yml2json:
            self.update_yml_field_list(y["components"]["schemas"][yk], data[jk]["spreadsheet_definition"])

        with open(file_path, "w") as out_file:
            yaml.dump(y, out_file)

    def update_main_yml(self, data, file_path):
        """Updates the dash openapi.yml file with various properties.
        TODO: default/required fields need to be annotated in upstream config and set here
        """
        if not os.path.exists(file_path):
            print(f"Skipping: {file_path} does not exist")
            return
        with open(file_path) as file:
            y = yaml.safe_load(file)

        self.update_yml_field_patterns(y, data)

        for (yk, jk, yk2) in self.yml2json:
            self.update_yml_field_list(
                y["components"]["schemas"][yk]["properties"][yk2]["items"], data[jk]["spreadsheet_definition"]
            )

        with open(file_path, "w") as out_file:
            yaml.dump(y, out_file)

    def update_all(self):
        """Runs updates on all metadata files."""
        metadata_path = f"{self.root_path}/metadata"
        for entry in os.scandir(metadata_path):
            if entry.is_dir():
                config_path = f"{metadata_path}/{entry.name}/config.json"
                if os.path.exists(config_path):
                    with open(config_path) as config_file:
                        data = json.load(config_file)
                        for (k, class_name) in [
                            ("metadata", "Metadata"),
                            ("in_silico_data", "InSilicoData"),
                            ("qc_data", "QCData"),
                        ]:
                            self.generate_dataclass_file(
                                data[k],
                                class_name,
                                f"{metadata_path}/{entry.name}/metadata/api/model/{k}.py",
                            )
                        for (k, filename) in [
                            ("metadata", "api_sample.sql"),
                            ("in_silico_data", "in_silico.sql"),
                            ("qc_data", "qc_data.sql"),
                        ]:
                            self.update_database_definition(data[k], f"{self.root_path}/database/tables/{filename}")
                        self.update_metadata_tests(data, f"{metadata_path}/{entry.name}/metadata/tests")
                        self.update_dash_yml(data, f"{self.root_path}/dash-api/{entry.name}/dash/interface/openapi.yml")
                        self.update_main_yml(data, f"{metadata_path}/{entry.name}/metadata/interface/openapi.yml")

    def write_field_attributes_file(self):
        """Generated the file_attributes.json file for dash-api."""
        field_attributes_file = f"{self.root_path}/dash-api/juno/field_attributes.json"
        field_attributes = copy.deepcopy(self.config)
        for _, kmc in self.map_config_dict.items():
            for k in self.config_additional_section_keys:
                if k in field_attributes[kmc]:
                    field_attributes[kmc].pop(k)
            for category in field_attributes[kmc]["categories"]:
                for fields in category["fields"]:
                    if "db" in fields:
                        fields.pop("db")
        json_object = json.dumps(field_attributes, indent=3)
        with open(field_attributes_file, "w") as out_file:
            out_file.write(json_object)

    def update_config_section(self, c, mc):
        """Updates a section of config data (c) from main config (mc)."""
        for k in self.config_additional_section_keys:
            if k in mc:
                c[k] = mc[k]

        sd = {}  # spreadsheet_definition
        for category in mc["categories"]:
            for field in category["fields"]:
                if "db" in field:
                    d = {}
                    if "spreadsheet heading" in field:
                        d["title"] = field["spreadsheet heading"]
                    else:
                        d["title"] = field["name"]
                    for k, v in field["db"].items():
                        d[k] = v
                    if "id" in d:
                        id = d.pop("id")
                    else:
                        id = field["name"]
                    sd[id] = d
        c["spreadsheet_definition"] = sd
        return c

    def update_config_json(self, config_path):
        """Updates a config.json file based on main_config.json"""
        with open(config_path) as config_file:
            config = json.load(config_file)

        for kc, kmc in self.map_config_dict.items():
            config[kc] = self.update_config_section(config[kc], self.config[kmc])

        json_object = json.dumps(config, indent=3)
        with open(config_path, "w") as out_file:
            out_file.write(json_object)

    def update_from_main_config(self):
        """Updates metadata/*/config./json files, as well as dash-api/juno/field_attributes.json.
        Required first step for further updates.
        """
        main_config_file = f"{self.root_path}/config/main_config.json"
        with open(main_config_file) as config_file:
            self.config = json.load(config_file)

        self.write_field_attributes_file()

        metadata_path = f"{self.root_path}/metadata"
        for entry in os.scandir(metadata_path):
            if entry.is_dir():
                config_path = f"{metadata_path}/{entry.name}/config.json"
                if os.path.exists(config_path):
                    self.update_config_json(config_path)


if __name__ == "__main__":
    umf = UpdateMetadataFiles()
    umf.update_from_main_config()
    umf.update_all()
